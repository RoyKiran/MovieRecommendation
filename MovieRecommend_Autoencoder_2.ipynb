{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders \n",
    "\n",
    "Predict movie rating 1-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                      # To work with arrays\n",
    "import pandas as pd                     # To import the data set and create the train & test set\n",
    "import torch\n",
    "import torch.nn as nn                   # To implement neural networks.\n",
    "import torch.nn.parallel                # For the parallel computations.\n",
    "import torch.optim as optim             # For the optimizers\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable     # For stochastic gradient descent in the set - Autograd is an engine to calculate derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is A2C9-9F3D\n",
      "\n",
      " Directory of C:\\Users\\Work Is Fun\\Desktop\\study\\courses\\Udemy-A_ZdeepLearning 2020\\Excercise-Deep_Learning\\Vol2-Unsupervised Deep Learning\\Part6 AutoEncoders (AE)\\Section 22 - Building an AutoEncoder\n",
      "\n",
      "08/29/2020  11:58 AM    <DIR>          .\n",
      "08/29/2020  11:58 AM    <DIR>          ..\n",
      "10/20/2017  07:02 PM             6,148 .DS_Store\n",
      "08/22/2020  01:05 PM    <DIR>          .ipynb_checkpoints\n",
      "08/29/2020  11:58 AM            17,975 Autoencoder_Tut12.ipynb\n",
      "               2 File(s)         24,123 bytes\n",
      "               3 Dir(s)  596,839,075,840 bytes free\n"
     ]
    }
   ],
   "source": [
    "!cd '../../Part5 Boltzmann Machines (BM)/'\n",
    "!dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\Work Is Fun\\Desktop\\study\\courses\\Udemy-A_ZdeepLearning 2020\\Excercise-Deep_Learning\\Vol2-Unsupervised Deep Learning\\Part5 Boltzmann Machines (BM)\\Sec19 - Building a BM\\\\'\n",
    "movies = pd.read_csv(filepath+\"ml-1m\\\\movies.dat\", sep='::', header=None, engine='python', encoding='latin-1' )\n",
    "users = pd.read_csv(filepath+\"ml-1m\\\\users.dat\", sep='::', header=None, engine='python', encoding='latin-1' )\n",
    "ratings = pd.read_csv(filepath+\"ml-1m\\\\ratings.dat\", sep='::', header=None, engine='python', encoding='latin-1' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(filepath+'ml-100k\\\\u1.base', delimiter='\\t', header=None)\n",
    "test_set = pd.read_csv(filepath+'ml-100k\\\\u1.test', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframes to array\n",
    "#training_set = np.array(training_set, dtype=int)\n",
    "training_set = np.array(training_set)\n",
    "test_set = np.array(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting total no. of users &movies\n",
    "nb_users = int(max(max(training_set[:][0]), max(test_set[:][0])))\n",
    "nb_movies = int(max(max(training_set[:][1]), max(test_set[:][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset where rows are the users,the columns are the movies, the cells are the ratings.\n",
    "def prepare_dataset(df:np.ndarray) -> list:\n",
    "    dataset = []\n",
    "    \n",
    "    for user_id in range(1,nb_users+1):\n",
    "        movie_ids = df[:][1][df[:][0]== user_id]\n",
    "        movie_ratings = df[:][2][df[:][0]== user_id]\n",
    "        \n",
    "        user_ratings = np.zeros(nb_movies)\n",
    "        user_ratings[movie_ids-1] = movie_ratings \n",
    "        \n",
    "        dataset.append(list(user_ratings))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = prepare_dataset(training_set)  # Convert training_set into desired array/matrix format\n",
    "test_set = prepare_dataset(test_set)          # Convert test_set into desired array/matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array/list to torch tensors\n",
    "training_set = torch.FloatTensor(training_set)  # FloatTensor(list_of_list) class creates a multi dimensional array/matrix with element data type as float\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Architecture of Neural Network - Stacked AutoEncoder\n",
    "class SAE(nn.Module):\n",
    "    '''\n",
    "    The parent class Module contains all the tools to make an auto-encoder - it contains an optimizer function, a criterion\n",
    "    & tools to make full connections between the layers.\n",
    "    In Stacked AutoEncoders, we have several hidden layers i.e. we will have several encodings of the input vector features.\n",
    "    '''\n",
    "    \n",
    "    # Initialize the object of the class - defines the architecture of our auto encoders\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Get all the inherited classes and methods of the parent class\n",
    "        super(SAE,self).__init__() \n",
    "        \n",
    "        # Define the neural architecture by choosing the number of layers & the hidden neurons in each of these hidden layers.\n",
    "        '''\n",
    "        In auto-encoders, first We're encoding the input vector into a shorter vector. This will take place in the \n",
    "        first hidden layer.\n",
    "        So we first establish a connection between the input vector features (the ratings of all the movies for one \n",
    "        specific user) & the first hidden layer, which is a shorter vector than the input vector.        \n",
    "        '''\n",
    "        # fcN = Fully Connected layer N\n",
    "        # nn.Linear(in_features, out_features, bias=True) : Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "        # ----- Encoding Layers -------\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)  # no. of neuron in input layer = nb_novies & no. of hidden neurons in first hidden layer = 20 (by r&d)\n",
    "        self.fc2 = nn.Linear(20,10)  # no. of hidden neurons in first hidden layer = 20 & no. of hidden neurons in 2nd hidden layer = 10 (by r&d)\n",
    "        \n",
    "        # ----- Decoding Layers -------\n",
    "        self.fc3 = nn.Linear(10,20)  # no. of hidden neurons in 2nd hidden layer = 10 & no. of hidden neurons in 3rd hidden layer = no. of hidden neurons in first hidden layer\n",
    "        self.fc4 = nn.Linear(20,nb_movies)  # no. of hidden neurons in 3rd hidden layer = no. of hidden neurons in first hidden layer & no. of output neurons = no. of input neurons\n",
    "        \n",
    "        # Define activation function - that will, activate the neurons when the observation goes into the network.\n",
    "        self.activation = nn.Sigmoid()   # You can use rectifier activation function or the sigmoid activation function\n",
    "        \n",
    "        \n",
    "    # Perform the action of an auto-encoder - encoding and decoding.\n",
    "    '''\n",
    "    This function will not only perform the action of encoding & decoding, but also will apply to different activation \n",
    "    functions inside the full connections.\n",
    "    The main purpose of making this function is that it will return in the end the vector of predicted ratings that \n",
    "    we will compare to the vector of real ratings, i.e. the input vector.\n",
    "    '''\n",
    "    def forward(self, vector_x):\n",
    "        \n",
    "        # Encoding\n",
    "        vector_x = self.activation(self.fc1(vector_x))  # Update input vector to 1st hidden layer vector of neurons\n",
    "        vector_x = self.activation(self.fc2(vector_x))  # Update 1st hidden layer vector of neurons to 2nd hidden layer vector of neurons\n",
    "        \n",
    "        # Decoding\n",
    "        vector_x = self.activation(self.fc3(vector_x))  # Update 2nd hidden layer vector of neurons to 3rd hidden layer vector of neurons\n",
    "        vector_x = self.fc4(vector_x)                   # Update 3rd hidden layer vector of neurons to output vector\n",
    "        \n",
    "        return vector_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom/User-defined Objects are callable. Refer [link](https://dbader.org/blog/python-first-class-functions#:~:text=Object%27s%20aren%27t%20functions%20in,like%20functions%20in%20many%20cases.&text=Behind%20the%20scenes%2C%20%E2%80%9Ccalling%E2%80%9D,object%27s%20__call__%20method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRMSprop class takes in 3 input arguments:\\n1) all the parameters of our AutoEncoders that define the architecture\\n     - no. of neurons in each layer (input, output, hidden) & the Sigmoid activation function.\\n   An attribute from our SAE object will get us all these parameters. -> sae.parameters()\\n\\n2) learning rate/ LR \\n3) decay/ weight_decay - used to reduce the learning rate after every few epochs in order to regulate the convergence.\\n   This parameter can improve your model even more.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = SAE()                  # Create autoencoder\n",
    "creterion = nn.MSELoss()     # Get the creterion to measure mean squared error\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)  # Define optimizer to apply stocastic gradient descent to update the \n",
    "                             # different weights in order to reduce the error at each epoch.\n",
    "                             # U can use Atom class for the Atom optimizer or RMS prop class for the RMS prop optimizer.\n",
    "'''\n",
    "RMSprop class takes in 3 input arguments:\n",
    "1) all the parameters of our AutoEncoders that define the architecture\n",
    "     - no. of neurons in each layer (input, output, hidden) & the Sigmoid activation function.\n",
    "   An attribute from our SAE object will get us all these parameters. -> sae.parameters()\n",
    "\n",
    "2) learning rate/ LR \n",
    "3) decay/ weight_decay - used to reduce the learning rate after every few epochs in order to regulate the convergence.\n",
    "   This parameter can improve your model even more.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t Average Loss: 0.922285158838978\n",
      "epoch: 2\t Average Loss: 0.9139579700134213\n",
      "epoch: 3\t Average Loss: 0.9114530043536254\n",
      "epoch: 4\t Average Loss: 0.9112882806286453\n",
      "epoch: 5\t Average Loss: 0.9113469857046991\n",
      "epoch: 6\t Average Loss: 0.9110108716363998\n",
      "epoch: 7\t Average Loss: 0.9103412972022468\n",
      "epoch: 8\t Average Loss: 0.9109248528424907\n",
      "epoch: 9\t Average Loss: 0.9101167680855647\n",
      "epoch: 10\t Average Loss: 0.9105088040785724\n",
      "epoch: 11\t Average Loss: 0.9095769014489992\n",
      "epoch: 12\t Average Loss: 0.9091301085713812\n",
      "epoch: 13\t Average Loss: 0.9088167775094446\n",
      "epoch: 14\t Average Loss: 0.9084796925743969\n",
      "epoch: 15\t Average Loss: 0.9078022864818067\n",
      "epoch: 16\t Average Loss: 0.9073882441758351\n",
      "epoch: 17\t Average Loss: 0.9098802649456522\n",
      "epoch: 18\t Average Loss: 0.9072815135579599\n",
      "epoch: 19\t Average Loss: 0.906076667923184\n",
      "epoch: 20\t Average Loss: 0.9065048848836824\n",
      "epoch: 21\t Average Loss: 0.905307612009544\n",
      "epoch: 22\t Average Loss: 0.9060476066451816\n",
      "epoch: 23\t Average Loss: 0.9050386171957847\n",
      "epoch: 24\t Average Loss: 0.9046284583568067\n",
      "epoch: 25\t Average Loss: 0.9033188238376524\n",
      "epoch: 26\t Average Loss: 0.9036162974093651\n",
      "epoch: 27\t Average Loss: 0.9022848507505965\n",
      "epoch: 28\t Average Loss: 0.9019785099292484\n",
      "epoch: 29\t Average Loss: 0.9014828500919605\n",
      "epoch: 30\t Average Loss: 0.9010823351960167\n",
      "epoch: 31\t Average Loss: 0.9006099761606906\n",
      "epoch: 32\t Average Loss: 0.9005530833692338\n",
      "epoch: 33\t Average Loss: 0.9000292683962421\n",
      "epoch: 34\t Average Loss: 0.899335940088978\n",
      "epoch: 35\t Average Loss: 0.8989929005045401\n",
      "epoch: 36\t Average Loss: 0.8990795018184982\n",
      "epoch: 37\t Average Loss: 0.8980955607312102\n",
      "epoch: 38\t Average Loss: 0.8980738133160459\n",
      "epoch: 39\t Average Loss: 0.8970274778590602\n",
      "epoch: 40\t Average Loss: 0.8969788697971898\n",
      "epoch: 41\t Average Loss: 0.8956470995161718\n",
      "epoch: 42\t Average Loss: 0.8966416554132424\n",
      "epoch: 43\t Average Loss: 0.8956613388951484\n",
      "epoch: 44\t Average Loss: 0.8948223158511731\n",
      "epoch: 45\t Average Loss: 0.8938040060809915\n",
      "epoch: 46\t Average Loss: 0.8931254349483033\n",
      "epoch: 47\t Average Loss: 0.8929190286775914\n",
      "epoch: 48\t Average Loss: 0.8924642746926366\n",
      "epoch: 49\t Average Loss: 0.8912981990033471\n",
      "epoch: 50\t Average Loss: 0.890958266192504\n",
      "epoch: 51\t Average Loss: 0.8905702431153897\n",
      "epoch: 52\t Average Loss: 0.890094647857569\n",
      "epoch: 53\t Average Loss: 0.8891738778333775\n",
      "epoch: 54\t Average Loss: 0.8892580843426896\n",
      "epoch: 55\t Average Loss: 0.888168318779825\n",
      "epoch: 56\t Average Loss: 0.8880679958824894\n",
      "epoch: 57\t Average Loss: 0.8874340198957781\n",
      "epoch: 58\t Average Loss: 0.8869074864958908\n",
      "epoch: 59\t Average Loss: 0.8869068392513918\n",
      "epoch: 60\t Average Loss: 0.8864689136234093\n",
      "epoch: 61\t Average Loss: 0.8853194721177425\n",
      "epoch: 62\t Average Loss: 0.8855005711285459\n",
      "epoch: 63\t Average Loss: 0.885076302359491\n",
      "epoch: 64\t Average Loss: 0.8853175951086957\n",
      "epoch: 65\t Average Loss: 0.8840377338364925\n",
      "epoch: 66\t Average Loss: 0.8849243940755899\n",
      "epoch: 67\t Average Loss: 0.8833708778292352\n",
      "epoch: 68\t Average Loss: 0.8834044698187301\n",
      "epoch: 69\t Average Loss: 0.8829240849516172\n",
      "epoch: 70\t Average Loss: 0.8826512713953142\n",
      "epoch: 71\t Average Loss: 0.8820987187665694\n",
      "epoch: 72\t Average Loss: 0.8828465450606442\n",
      "epoch: 73\t Average Loss: 0.8817686240721103\n",
      "epoch: 74\t Average Loss: 0.8813292097817802\n",
      "epoch: 75\t Average Loss: 0.8808244237970573\n",
      "epoch: 76\t Average Loss: 0.8804112876333842\n",
      "epoch: 77\t Average Loss: 0.8810165259643425\n",
      "epoch: 78\t Average Loss: 0.8804904456356044\n",
      "epoch: 79\t Average Loss: 0.8801870174145016\n",
      "epoch: 80\t Average Loss: 0.8801197039866119\n",
      "epoch: 81\t Average Loss: 0.8789597771200623\n",
      "epoch: 82\t Average Loss: 0.8788764120285989\n",
      "epoch: 83\t Average Loss: 0.8807654598232039\n",
      "epoch: 84\t Average Loss: 0.879601455316311\n",
      "epoch: 85\t Average Loss: 0.8769716361926697\n",
      "epoch: 86\t Average Loss: 0.8771962300338016\n",
      "epoch: 87\t Average Loss: 0.8762416091223157\n",
      "epoch: 88\t Average Loss: 0.8766337745642232\n",
      "epoch: 89\t Average Loss: 0.8761858166465072\n",
      "epoch: 90\t Average Loss: 0.8764567531937633\n",
      "epoch: 91\t Average Loss: 0.8757047198104454\n",
      "epoch: 92\t Average Loss: 0.8759838116383881\n",
      "epoch: 93\t Average Loss: 0.874866991255468\n",
      "epoch: 94\t Average Loss: 0.8748836254390907\n",
      "epoch: 95\t Average Loss: 0.8736183271681137\n",
      "epoch: 96\t Average Loss: 0.8745823331248344\n",
      "epoch: 97\t Average Loss: 0.8734990400069592\n",
      "epoch: 98\t Average Loss: 0.8745062818962089\n",
      "epoch: 99\t Average Loss: 0.8739833730655157\n",
      "epoch: 100\t Average Loss: 0.8738302350170666\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE\n",
    "nb_epoch = 100    # No. of epochs\n",
    "\n",
    "# Loop over all the epochs\n",
    "for epoch in range(1,nb_epoch+1):\n",
    "    \n",
    "    train_loss = 0       # Initialize training error loss\n",
    "    s = 0.               # Initialize a variable to save memory by ignoring the users who did not rate any movie\n",
    "                         # Using this variable we keep track of no. of users who rated\n",
    "                         # This is a float variable as we will use this to caculate the Root Mean Squared Error\n",
    "    \n",
    "    # Loop over all the observations i.e. users\n",
    "    for id_user in range(nb_users):\n",
    "        \n",
    "        # Get input vector of features that contains all the ratings of all the movies given by a user\n",
    "        # Here we also specify where we want add an axis.\n",
    "        input_vector = Variable(training_set[id_user]).unsqueeze(0)   # unsqueeze(new_index) inserts singleton dim at position given as new_index & return the tensor\n",
    "        '''\n",
    "        training set[id_user] is a vector of 1D and a network in PyTorch or even on Kerras generally can not \n",
    "        accept a single vector of one dimension. Rather it accepts a batch of input vectors. So We add an additional \n",
    "        dimension like a fake dimension, which will correspond to a batch, by specifying the index of this new dimension.\n",
    "        We put this new dimension in first position (at index zero).\n",
    "        Variable().unsqueeze(index) will create a batch of, a single input vector. The batch can have several input vectors.\n",
    "        But here since we are not doing batch learning but online learning (we'll update the weights after each observation\n",
    "        going to the network), we create a batch of one input vector.\n",
    "        '''\n",
    "        \n",
    "        # Before the input vector updates, We would need a clone of it for comparison at the target side\n",
    "        target_vector = input_vector.clone()\n",
    "        \n",
    "        # Make Prediction\n",
    "        # Ignore users who did not even vote 1 movie\n",
    "        if torch.sum(target_vector.data >0) > 0:    \n",
    "            \n",
    "            # Get vector of our predicted ratings\n",
    "            output_predicted = sae(input_vector)    # OR    sae.forward(input_vector)\n",
    "        \n",
    "            # Perform Optimization for optimizing the memory and the computations.\n",
    "            ''' \n",
    "            1. When we apply stochastic gradient descent, we want to make sure the gradient is computed only with respect \n",
    "            to the input and not the target. This will same computation & memory.\n",
    "            requires_grad: This member, if true starts tracking all the operation history and forms a backward graph for \n",
    "            gradient calculation.\n",
    "            '''\n",
    "            target_vector.require_grad = False\n",
    "            \n",
    "            '''\n",
    "            2.  We also don't deal with the movies that the user didn't rate, where the ratings are equal to zero,\n",
    "            but that is only for the output vector.\n",
    "            So using the indexes for the movies that has not been rated by a user i.e. the input vector = 0, we set the  \n",
    "            rating of these movies to 0 in the output vector.\n",
    "            While updating the weights these values won't count.\n",
    "            '''\n",
    "            output_predicted[target_vector == 0] = 0\n",
    "            \n",
    "            \n",
    "            # Loss Computation\n",
    "            loss = creterion(output_predicted,target_vector)   # Find loss by using (predicted_outcome, expected_outcome)\n",
    "            \n",
    "            mean_corrector = nb_movies/float(torch.sum(target_vector.data > 0) +1e-10)  # (number of movies/ number of movies that have positive ratings)\n",
    "            '''\n",
    "            mean_corrector represents the average of the error, but by only considering the movies that were rated i.e. \n",
    "            the movies that at least got one to five ratings.\n",
    "            Since, no. of movies that have positive ratings may be = 0, we add the demoniator to a very tiny value 1e-10, \n",
    "            to avoid divide by 0 error & also not create any bias to the calculation.\n",
    "            '''\n",
    "            \n",
    "            # Backpropogation / Backward method for the loss - this will tell in which direction we need to update the\n",
    "            # different weights. i.e. do we need to increase the weight or decrease the weight.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute RMSE                        \n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)  # Sum up the error (difference between predicted & expected rating)\n",
    "                                                       # we're adjusting this loss with this mean corrector factor \n",
    "            \n",
    "            s += 1.        # Increment s to denote user has rated atleast one movie\n",
    "            \n",
    "            # Apply optimizer to update the weights.\n",
    "            '''            \n",
    "            After we've measured the error, the weights are updated by the RMSprop Optimizer.\n",
    "            Backward operation provides the direction in which the weights should be updated & optimizer decides the \n",
    "            intensity of the updates i.e. the amount by which the weights will be updated.\n",
    "            '''            \n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "    # Check the loss at each epoch    \n",
    "    print(\"epoch: \"+str(epoch) + \"\\t Average Loss: \"+str(train_loss.item()/s))\n",
    "    '''\n",
    "    To print value in a pytorch tensor, there are 2 ways:\n",
    "    1) loss.numpy()[0]\n",
    "    2) loss.item() -> Valid only for tensor with 1 dimension\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.9588069209346065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif, on average, our model predicts a rating that will be different from the real rating by less than one star.\\nThen, that means that our recommended system will be pretty powerful.\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the SAE\n",
    "test_loss = 0        # Initialize test loss\n",
    "s = 0.               # Initialize a variable to save memory by ignoring the users who did not rate any movie\n",
    "                     # Using this variable we keep track of no. of users who rated\n",
    "                     # This is a float variable as we will use this to caculate the Root Mean Squared Error\n",
    "\n",
    "# Loop over all the observations i.e. users\n",
    "for id_user in range(nb_users):\n",
    "\n",
    "    # Get input vector of features that contains the ratings of all the movies given by a user\n",
    "    # Here we also specify where we want add an axis.\n",
    "    input_vector = Variable(training_set[id_user]).unsqueeze(0)   # unsqueeze(new_index) inserts singleton dim at position given as new_index & return the tensor\n",
    "    '''\n",
    "    The test set contains those ratings which are not rated in the training set. Our motto is to predict these movies \n",
    "    which are not rated in the training set based on the ratings given by the user to other movies. Hence, here we\n",
    "    pick the users from training set & not test set.\n",
    "    \n",
    "    So, the training set is like, the set that contains all the ratings of the movies by a specific user, \n",
    "    up to a certain point in time. Then we predict the ratings of the other movies that the user hasn't watched yet,\n",
    "    and then we have the test set in the future that contains the real answers, like it contains the real ratings \n",
    "    for these movies that were not part of the training set.\n",
    "    '''\n",
    "\n",
    "    # Expected outcome \n",
    "    target_vector = Variable(test_set[id_user]).unsqueeze(0) \n",
    "\n",
    "    # Make Prediction\n",
    "    # Ignore users who did not even vote 1 movie\n",
    "    if torch.sum(target_vector.data >0) > 0:    \n",
    "\n",
    "        # Get vector of our predicted ratings\n",
    "        output_predicted = sae(input_vector)   # OR    sae.forward(input_vector)\n",
    "        '''\n",
    "        SAE contains the forward() that returns the vector of predicted ratings. Thus, by calling our object on the\n",
    "        input here, we will get our vector of predicted ratings for the movies that the user hasn't watched yet,\n",
    "        and this will go into output.\n",
    "        '''\n",
    "\n",
    "        # Perform Optimization for optimizing the memory and the computations.\n",
    "        '''override the computations of the gradient with respect to the target '''\n",
    "        target_vector.require_grad = False\n",
    "\n",
    "        '''consider the ratings of the movies that are non-zero ratings in the test set \n",
    "         & we don't measure the future loss on the movies that didn't get any rating. '''\n",
    "        output_predicted[target_vector == 0] = 0  \n",
    "\n",
    "\n",
    "        # Loss Computation\n",
    "        loss = creterion(output_predicted,target_vector)   # Find loss by using (predicted_outcome, expected_outcome)\n",
    "\n",
    "        mean_corrector = nb_movies/float(torch.sum(target_vector.data > 0) +1e-10)  # (number of movies/ number of movies that have positive ratings)\n",
    "        '''\n",
    "        mean_corrector represents the average of the error, but by only considering the movies that were rated i.e. \n",
    "        the movies that at least got one to five ratings.\n",
    "        Since, no. of movies that have positive ratings may be = 0, we add the demoniator to a very tiny value 1e-10, \n",
    "        to avoid divide by 0 error & also not create any bias to the calculation.\n",
    "        '''       \n",
    "\n",
    "        # Compute RMSE                        \n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)  # Sum up the error (difference between predicted & expected rating)\n",
    "                                                   # we're adjusting this loss with this mean corrector factor \n",
    "\n",
    "        s += 1.        # Increment s to denote user has rated atleast one movie\n",
    "\n",
    "\n",
    "\n",
    "# Check the test loss    \n",
    "print(\"Average Test Loss: \"+str(test_loss.item()/s))\n",
    "'''\n",
    "if, on average, our model predicts a rating that will be different from the real rating by less than one star.\n",
    "Then, that means that our recommended system will be pretty powerful.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
